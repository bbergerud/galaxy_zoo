\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Proposal Template for Machine Learning Course}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  XXX\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  University of Iowa\\
  Iowa City, IA  52242 \\
  \texttt{xxx@uiowa.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
With the introduction of powerful telescopes such as the Hubble Space Telescope, vast quantities of high-fidelity imagery of remote galaxies have become available. Manual analysis of these images by experts has become infeasible, spawning citizen science projects such as Galaxy Zoo. However, the next generation of telescopes are expected to generate enormous volumes of data, going far beyond the capacity even of crowdsourced volunteers. In this study, we will extend the work done on automatic galaxy image classification in the Galaxy Zoo Kaggle challenge by developing a mapping between the various Galaxy Zoo "classification trees" and the popular Hubble Tuning Fork model. We will build a convolutional neural network to classify galaxies by leveraging the various crowdsourced Galaxy Zoo "gold standard" datasets. The model will be tested against expert-annotated classifications using third-party images.
\end{abstract}

\section{Introduction}
The size and scope of astronomy datasets has increased dramatically in recent years. The introduction of telescopes such as the Hubble Space Telescope (HST) and projects like the Sloan Digital Sky Survey (SDSS) have given astronomers access to imagery of millions of celestial objects. Traditional methods of data analysis, manually inspecting and classifying celestial objects, have become untenable in the face of this embarrassment of riches of data. 

Astronomers have successfully turned to citizen science projects such as Galaxy Zoo to leverage vast numbers of volunteers to help classify objects. The human visual system can, with little effort or training, provide image recognition capabilities that match or exceed the state of the art in computer image recognition. 

With the dawn of a new generation of telescopes, astronomy is threatened to be deluged in a sea of data. The GAIA spacecraft will produce a 3D map of over 1 billion astronomical objects. The Thirty Meter Telescope (TMT) and the 40-meter European Extremely Large Telecope (E-ELT) will view the visible universe at unprecedented depth. The Large Synoptic Survey Telescope (LSST) is estimated to generate 15 TB of data each night as it surveys the entire sky. Even these vast sums of data pale in comparision to the 1 TB/s output expected from the monsuvian Square Kilometer Array (SKA), which isn't limited to night time observing. Such enormous sums of data are beyond the ability of crowdsourcing to handle: they can only be handled by leveraging supercomputers, sophisticated algorithms, and machine learning.

The Galaxy Zoo Kaggle challenge was a competition in 2013 to produce a machine learning model that could replicate the classifications of citizen science volunteers on a dataset of 70000 galaxy images captured by HST. The top models performed very well in this challenge, but several questions remain. Can the galaxy classification scheme used by Galaxy Zoo be effectively mapped to astronomical classification schemes such as Hubble's Tuning Fork, or the more complex de Vaucouleurs system? Will machine learning models trained on the Galaxy Zoo dataset generalize well to other sources? 

To answer these questions, we will develop a mapping system between the various Galaxy Zoo “decision tree” classification schemes and the Hubble Tuning Fork scheme. We will develop a machine learning system to produce Tuning Fork classifications and train it on data from the Galaxy Zoo projects. We will then locate 3rd party datasets of expert-annotated galaxy images and test our system on these images. This project will investigate the generalizability of the Galaxy Zoo training data and the feasibility of mapping between the two galaxy classification schemes. 


\section{Related Work}
\label{gen_inst}
In the astronomical community, the few automated galaxy classification systems have relied on more tradition methods, focusing on aggressive feature extraction algorithms making use of domain knowledge (such as WND-CHARM) to identify relationships among galaxies. These, however, have tended to focus on the more narrow classification of spirals and ellipticals, occasionally including edge-on spirals and irregular galaxies, and often work with muuch smaller datasets.

One example of the simple classification approach was done by , who, rather uniquely, made use of the ``super clean" galaxies from the Galaxy Zoo 1 catalog \citep{2008MNRAS.389.1179L} to classify 900 000 galaxies into spirals and ellipticals. They made use of an algorithm that extracted 2885 numerical descriptions from each image.

The Galaxy Zoo Kaggle challenge showed the power of convolutional neural networks (CNNs) when it comes to galaxy classification. Rather than relying on domain knowledge, the models had to learn to identify features on their own and were able to successfully reproduce the probabililty distributions of the citizen scientists. The winning model created 16 transformations for each image through the use of rotations and translations and trained the model on all 16 at once using convolutional layers and pooling.


\subsection{Citations within the text}

The \verb+natbib+ package will be loaded for you by default.
Citations may be author/year or numeric, as long as you maintain
internal consistency.  As to the format of the references themselves,
any style is acceptable as long as it is used consistently.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may
add the following before loading the \verb+nips_2016+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add
the optional argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{nips_2016}
\end{verbatim}

As submission is double blind, refer to your own published work in the
third person. That is, use ``In the previous work of Jones et
al.\ [4],'' not ``In our previous work [4].'' If you cite your other
papers that are not widely available (e.g., a journal paper under
review), use anonymous author names in the citation, e.g., an author
of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote,
indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Place the footnotes at the bottom of the
page on which they appear.  Precede the footnote with a horizontal
rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction. The figure number and caption
always appear after the figure. Place one line space before the figure
caption and one line space after the figure. The figure caption should
be lower case (except for first word and proper nouns); figures are
numbered consecutively.

You may use color figures.  However, it is best for the figure
captions and the paper body to be legible if the paper is printed in
either black/white or in color.
\begin{figure}[h]
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table
number and title always appear before the table.  See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical
  rules.} We strongly suggest the use of the \verb+booktabs+ package,
which allows for typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}[t]
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule{1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{The Proposed Work}
Describe your proposed work in this section. 



\section{Plan}
Describe your plan for the project. What data you are going to use to evaluate your methods? What are the baselines that you want to compare? How will you develop your methods?   A timeline with important milestones is always perferred. 


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level
heading for the references. Any choice of citation style is acceptable
as long as you are consistent. It is permissible to reduce the font
size to \verb+small+ (9 point) when listing the references.



\medskip

\small

\begin{thebibliography}

@ARTICLE{2016ApJS..223...20K,
   author = {{Kuminski}, E. and {Shamir}, L.},
    title = "{A Computer-generated Visual Morphology Catalog of \~{}3,000,000 SDSS Galaxies}",
  journal = {ApJS},
archivePrefix = "arXiv",
   eprint = {1602.06854},
 keywords = {catalogs, methods: data analysis, techniques: image processing},
     year = 2016,
    month = apr,
   volume = 223,
      eid = {20},
    pages = {20},
      doi = {10.3847/0067-0049/223/2/20},
   adsurl = {http://adsabs.harvard.edu/abs/2016ApJS..223...20K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.450.1441D,
   author = {{Dieleman}, S. and {Willett}, K.~W. and {Dambre}, J.},
    title = "{Rotation-invariant convolutional neural networks for galaxy morphology prediction}",
  journal = {MNRAS},
archivePrefix = "arXiv",
   eprint = {1503.07077},
 primaryClass = "astro-ph.IM",
 keywords = {methods: data analysis, techniques: image processing, catalogues, galaxies: general},
     year = 2015,
    month = jun,
   volume = 450,
    pages = {1441-1459},
      doi = {10.1093/mnras/stv632},
   adsurl = {http://adsabs.harvard.edu/abs/2015MNRAS.450.1441D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014arXiv1409.4842S,
   author = {{Szegedy}, C. and {Liu}, W. and {Jia}, Y. and {Sermanet}, P. and 
	{Reed}, S. and {Anguelov}, D. and {Erhan}, D. and {Vanhoucke}, V. and 
	{Rabinovich}, A.},
    title = "{Going Deeper with Convolutions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1409.4842},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = 2014,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.4842S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2013MNRAS.435.2835W,
   author = {{Willett}, K.~W. and {Lintott}, C.~J. and {Bamford}, S.~P. and 
	{Masters}, K.~L. and {Simmons}, B.~D. and {Casteels}, K.~R.~V. and 
	{Edmondson}, E.~M. and {Fortson}, L.~F. and {Kaviraj}, S. and 
	{Keel}, W.~C. and {Melvin}, T. and {Nichol}, R.~C. and {Raddick}, M.~J. and 
	{Schawinski}, K. and {Simpson}, R.~J. and {Skibba}, R.~A. and 
	{Smith}, A.~M. and {Thomas}, D.},
    title = "{Galaxy Zoo 2: detailed morphological classifications for 304 122 galaxies from the Sloan Digital Sky Survey}",
  journal = {MNRAS},
archivePrefix = "arXiv",
   eprint = {1308.3496},
 keywords = {methods: data analysis, catalogues, galaxies: elliptical and lenticular, galaxies: general, galaxies: spiral},
     year = 2013,
    month = nov,
   volume = 435,
    pages = {2835-2860},
      doi = {10.1093/mnras/stt1458},
   adsurl = {http://adsabs.harvard.edu/abs/2013MNRAS.435.2835W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008MNRAS.389.1179L,
   author = {{Lintott}, C.~J. and {Schawinski}, K. and {Slosar}, A. and {Land}, K. and 
	{Bamford}, S. and {Thomas}, D. and {Raddick}, M.~J. and {Nichol}, R.~C. and 
	{Szalay}, A. and {Andreescu}, D. and {Murray}, P. and {Vandenberg}, J.
	},
    title = "{Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey}",
  journal = {MNRAS},
archivePrefix = "arXiv",
   eprint = {0804.4483},
 keywords = {methods: data analysis , galaxies: elliptical and lenticular, cD , galaxies: general , galaxies: spiral},
     year = 2008,
    month = sep,
   volume = 389,
    pages = {1179-1189},
      doi = {10.1111/j.1365-2966.2008.13689.x},
   adsurl = {http://adsabs.harvard.edu/abs/2008MNRAS.389.1179L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




\end{thebibliography}

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
